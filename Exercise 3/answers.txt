Q&A of Exercise 3

1.
Q: When smoothing the CPU temperature, do you think you got a better result with LOESS or Kalman smoothing? What differences did you notice?
A: I think I got a better result with the Kalman smoothing. According to the output plot, it is obvious that the LOESS smoothing is easier to get affected by outliers than the Kalman smoothing (i.e. LOESS is more outlier sensitivity). And for the parts that do not contain outliers, the output of two smoothings are similar, and the Kalman smoothing even has a lower sum of residuals (i.e. Kalman is more precise), thanks to the parameters tuning. 

2.
Q: In the GPX files, you might have also noticed other data about the observations: timestamp, course (heading in degrees from north, 0–360), speed (in m/s). How could those have been used to make a better prediction about the “next” latitude and longitude? [Aside: I tried, and it didn't help much. I think the values are calculated from the latitude/longitude by the app: they don't really add much new information.]
A: We need to decrease the observation error on variables of timestamp, course (heading in degrees from north, 0–360), speed (in m/s), by increasing the precision of observation or using other algorithms, to increase the quality of prediction.
According to the result of parameter tuning as well as the instruction, it is obvious that the transition covariance, which represents estimation error, is very small. This phenomenon indicates that the estimated value after the process of the Kalman filtering would be very close to the observation value. And therefore, the observation error, which is represented as the observation_covariance in the KalmanFilter() generator, determines whether we could make a good prediction. Given that observation covariance comes from the time stamp, course (heading in degrees from north, 0–360), speed (in m/s) in this exercise, we need to further reduce the observation error of these variables. Otherwise, the process of the Kalman filtering would lead to an extremely smooth result, which may lead to loss of information of data (as all the data would be similar), and trigger issues in further data analysis. 
